{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__fett__\n",
    "<br>\n",
    "_kursiv_\n",
    ">indent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung\n",
    "\n",
    "sinnvoll?{\n",
    "- Tutorial: Reinforcement Learning wodurch ein Agent das Navigieren in einer vorgegebenen Umgebung lernt.\n",
    "- In diesem Fall Spielumgebung. (gym / retro gym) (hier oder zweites Tut?)\n",
    "- Agent weiß zu Beginn nichts über die Umgebung die das Spiel darstellt und muss mittels \"Trial and Error\" lernen.\n",
    "- Einzigsten verfügbaren Informationen für Agent sind der visuelle Output des Spiels, die Eingabemöglichkeiten und ob einer zuvor getätigte Aktion zu einem Reward oder einer Penalty führt.\n",
    "}\n",
    "<br>\n",
    "<br>\n",
    "- Warum ein Spiel als Beispiel? \n",
    "\n",
    "- Was muss Agent leisten?\n",
    "    - Muss einzelne Spielelemente erkennen und unterscheiden\n",
    "    - Muss Auftreten einzelner Elemente auf dem Bildschirm mit den eigenen Aktionen und dem darauf folgenden Reward/Penalty in Verbindung bringen, welche auch erst nach mehreren Schritten in der Zukunft liegen können.\n",
    "\n",
    "<br>\n",
    "- In diesem Notebook: Wie beurteilt Agent Reward etc. (Q-Value), wenn er ein Bild der Spielumgebung sieht. Q-Value zeigt Agent, welche Aktion am wahrscheinlichsten zu dem höchsten kummulativen Reward (in der Zukunft) führt. Das Problem wird auf das finden des höchsten Q-Values reduziert -> Agent angepasst.\n",
    "\n",
    "<br>\n",
    "Daten: state-action pairs\n",
    "Ziel: Maximize future rewards over many time steps\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Voraussetzung: Kenntnisse: CNN, tensoflow (keras)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent, Spielumgebung etc. erklären"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konzept\n",
    "\n",
    "http://simplecore-dev.intel.com/ai/wp-content/uploads/sites/71/OpenAI-Gym-integration.png\n",
    "\n",
    "oder Folie 16 (MIT Vortrag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem (warum Q-Learning?)\n",
    "\n",
    "- Mensch: \n",
    "    - herausfinden, was was macht? (Controls etc.) \n",
    "    - Ziel herausfinden\n",
    "    - herausfinden und lernen von negativen Einflüssen und vermeiden\n",
    "    \n",
    "## Q-Learning\n",
    "\n",
    "vergehen mehrere Timesteps bevor Reward eintritt.\n",
    "(Breakout: Ball zurückschleudern -> dauert bis Ball ankommt. -> mehrere Zwischenzustände)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Einfachste Variante Reinforcement Learning umzusetzten -> Q-Learning.\n",
    "Hierfür werden sogenannte Q-Value oder Action-Values \"geschätzt\" (estimated). Diese weisen einem Zustand der Spielumgebung einen nummerischen Wert für jede mögliche Aktion des Agenten zu. Der Q-Value liefert dann eine \"Abschätzung\" darüber, welche Aktion am wahrscheinlichsten zu dem höchsten zukünftigen Reward führt. Auf diese Weise wird die entsprechende Aktion für den Agenten gewählt.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Die Q-Values sind zu Begin nicht bekannt und müssen irgendwie eingeschätzt werden.\n",
    "- werden mit Null initialisiert und wiederholt aktualisiert, wenn der Agent neue Infos sammelt.\n",
    "- Erziehlt der Agent Punkte, muss der Q-Value aktualisiert werden.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Die einfachste Variante Q-Value zu berechnen: Q-Value = Reward der beobachtet wurde + maximalen Q-Value für den folgenden Zustand des Spiels * Discount-Factor.\n",
    "-> So werden Rewards in näherer Zukunft vom Agent bevorzugt. \n",
    "\n",
    "$Q(s_t, a_t) = r_t + \\gamma * \\max\\limits_{a} Q(s_{t+1}, a)$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Agent verliert/stirbt -> zukünftiger Reward = 0 -> Q-Value = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Policy Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
