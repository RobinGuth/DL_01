{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__fett__\n",
    "<br>\n",
    "_kursiv_\n",
    ">indent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit\n",
    "\n",
    "### Requirements:\n",
    "- Basic understanding of reinforcement learning (RL)\n",
    "- Basic understanding of deep learning in terms of RL\n",
    "- keras\n",
    "- numpy\n",
    "...\n",
    "\n",
    "### Target\n",
    "- Understanding {Agent, Environment, Action}\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement learning (RL) is considered one of the three machine learning paradigms, like supervised learning and unsupervised learning. The task of the topic of RL is to navigate an agent by actions in an environment in a way to maximize a representation of a cummulativ reward. In order to miximize that reward the agent has to learn by trial and error which actions most likely result in a future reward and which lead to a penalty. An example of an environment is an game environment. The informations available for agent are the visual output of the game, the inpuroptions and whether a previous action results in a reward or a penalty.  \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "- Warum ein Spiel als Beispiel? \n",
    "\n",
    "- Was muss Agent leisten?\n",
    "    - Muss einzelne Spielelemente erkennen und unterscheiden\n",
    "    - Muss Auftreten einzelner Elemente auf dem Bildschirm mit den eigenen Aktionen und dem darauf folgenden Reward/Penalty in Verbindung bringen, welche auch erst nach mehreren Schritten in der Zukunft liegen können.\n",
    "\n",
    "<br>\n",
    "- In diesem Notebook: Wie beurteilt Agent Reward etc. (Q-Value), wenn er ein Bild der Spielumgebung sieht. Q-Value zeigt Agent, welche Aktion am wahrscheinlichsten zu dem höchsten kummulativen Reward (in der Zukunft) führt. Das Problem wird auf das finden des höchsten Q-Values reduziert -> Agent angepasst.\n",
    "\n",
    "<br>\n",
    "Daten: state-action pairs\n",
    "Ziel: Maximize future rewards over many tme steps\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "exploration vs exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent, Spielumgebung etc. erklären"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "total reward -> future reward ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The concept of RL\n",
    "\n",
    "In order to understand the image below properly we're given the following definitions:\n",
    "- Agent: takes actions\n",
    "- Environment: the world in which the agent exists and operates\n",
    "- Action ($A_{t}$): a move the agent can make in the environment\n",
    "- State ($S_{t}$): a situation which the agent preceives, a representation of the actual picture of the environment\n",
    "- Observation ($S_{t+1}$): the state of the environment after taking actions \n",
    "- Reward($R_{t}$): feedback that measures the success or failure of the agent's actions\n",
    "\n",
    "![Image of RLConcept](img/1-aKYFRoEmmKkybqJOvLt2JQ.png \"RL concept diagram\")\n",
    "Source: https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "The task of the agent is the maximization of a cumulative future reward. An example of this could be the score in a game. In a game not every single move leads to an immediate reward, so the agent needs to learn multiple moves in succession in order to fulfill the given task. For example, if the agent needs to collect an item in order to get a highter score, it may be necessary to take multiple moves to reach that item. So the moves in between doesn't increase the reward significantly, however they are needed to complete the task. \n",
    "\n",
    "In order to do the above we need the _total reward_ :\n",
    "\n",
    "$R_t = \\sum\\limits_{i=t}^{\\infty} r_i = r_t + r_{t+1} + ... + r_{t+n} + ...$\n",
    "\n",
    "$R_t = \\sum\\limits_{i=t}^{\\infty} \\gamma _i r_i = \\gamma _{t} r_t + \\gamma _{t+1} r_{t+1} + ... + \\gamma _{t+n} r_{t+n} + ...$\n",
    "\n",
    "\n",
    "\n",
    "### Q-function\n",
    "Rt dicounted summ of all rewards obtained from time t\n",
    "Q-function captures the expected total future reward (DQN)\n",
    "estemating Q-Value -> DQN\n",
    "\n",
    "### Value Learning\n",
    "\n",
    "### Policy Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem (warum Q-Learning?)\n",
    "\n",
    "- Mensch: \n",
    "    - herausfinden, was was macht? (Controls etc.) \n",
    "    - Ziel herausfinden\n",
    "    - herausfinden und lernen von negativen Einflüssen und vermeiden\n",
    "    \n",
    "## Q-Learning\n",
    "\n",
    "vergehen mehrere Timesteps bevor Reward eintritt.\n",
    "(Breakout: Ball zurückschleudern -> dauert bis Ball ankommt. -> mehrere Zwischenzustände)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Einfachste Variante Reinforcement Learning umzusetzten -> Q-Learning.\n",
    "Hierfür werden sogenannte Q-Value oder Action-Values \"geschätzt\" (estimated). Diese weisen einem Zustand der Spielumgebung einen nummerischen Wert für jede mögliche Aktion des Agenten zu. Der Q-Value liefert dann eine \"Abschätzung\" darüber, welche Aktion am wahrscheinlichsten zu dem höchsten zukünftigen Reward führt. Auf diese Weise wird die entsprechende Aktion für den Agenten gewählt.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Die Q-Values sind zu Begin nicht bekannt und müssen irgendwie eingeschätzt werden.\n",
    "- werden mit Null initialisiert und wiederholt aktualisiert, wenn der Agent neue Infos sammelt.\n",
    "- Erziehlt der Agent Punkte, muss der Q-Value aktualisiert werden.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Die einfachste Variante Q-Value zu berechnen: Q-Value = Reward der beobachtet wurde + maximalen Q-Value für den folgenden Zustand des Spiels * Discount-Factor.\n",
    "-> So werden Rewards in näherer Zukunft vom Agent bevorzugt. \n",
    "\n",
    "$Q(s_t, a_t) = r_t + \\gamma * \\max\\limits_{a} Q(s_{t+1}, a)$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Agent verliert/stirbt -> zukünftiger Reward = 0 -> Q-Value = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Policy Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
