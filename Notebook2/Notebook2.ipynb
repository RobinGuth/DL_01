{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Beginners Tutorial - Practice\n",
    "\n",
    "After taking a deeper look a the theory behind Reinforcement Learning, we will now build our first RL agent. !!TODO!! ADD MORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Targets](#targets)\n",
    "1. [OpenAi Gym](#openaigym)\n",
    "    1. [Install OpenAi Gym - required!](#installgym)\n",
    "    1. [Introduction to cartpole](#icartpole)\n",
    "    1. [Make epochs visible - required!](#epochvisible)\n",
    "    1. [Example of OpenAi Gym and rendering](#rendering)\n",
    "1. [Outlook](#outlook)\n",
    "1. [Sources](#sources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targets <a name=\"targets\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAi Gym <a name=\"openaigym\"></a>\n",
    "\n",
    "As we discussed before, Reinforcement Learning can be used to solve a range of different problems. Developing Machine Learning algorithms is often not easy to understand nor comprehensible especially for beginners. Furthermore, it is important to be able to compare the performance of different iterations of our algorithm, to be able to improve it. \n",
    "\n",
    "So bascially we need an environment, that we can use to test and train our RL agent, which fulfills the following requirements:\n",
    "\n",
    "- repeatable test/training epochs\n",
    "- finite set of inputs\n",
    "- finite set of actions\n",
    "- easy state representation\n",
    "- easy to control agent\n",
    "- deliver a score for a given state\n",
    "\n",
    "In practice, not all of these points will be fulfilled, but as this is a beginners guide, we will start with a simple environment. Luckily, many video games can be used as quite good environments for machine learning purposes.  Many implementations of RL are tested with games as Benchmark. There are some good reasons for this. Developing a whole test environment would be labour intensive and would require dedicated work towards a useable simulator. Using an existing game is also easier to compare to human performance and therefore the evaluation of different algorithms is easier. Another important point is the size of possible inputs and actions. The AI replaces the human player. Depending on the game, the input for our agent is an image, like a human player would see it. The set of actions is a combination of different buttons, which can be pressed on a controller. Finally, games are fun and most people can relate to them. It is also easier to understand what we want to accomplish, because we can transfer aspects from our human play style to the behaviour of an AI.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install OpenAiGym - required! <a name=\"installgym\"></a>\n",
    "\n",
    "We have to install OpenAi Gym by executing the following code. This will install Gym itself and all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to cartpole <a name=\"icartpole\"></a>\n",
    "\n",
    "As a first step in the creation of an AI, we should always look at our environment, to better understand what we want to achieve with the algorithm. The game, which will be used as environment, is called Cartpole. The following information is based of source [1](#sources). It involes a pretty simple task: The player tries to balance a pole in a 2D world without letting it tip over. We can play this game in the real world with something like a broomstick. This may seem trivial at first, but this task gets much harder if the pole is short. If we try the same with a pen for example, we will likely fail to balance it for a longer period of time. In the game, difficulty is reached by making the pole very sensitive to not beeing perpendicular to the ground and accelerating very fast. Instead of our hand, the pole is resting on a small cart, that the player can move right or left. Our RL algorithm will replace the player completely and will have to do all tasks a human player would need to do. An image of Cartpole is shown below.\n",
    "\n",
    "![Image of Cartpole](img/cartpole01.PNG \"Exemplary Cartpole\")\n",
    "\n",
    "Cartpole is an endless game and there are only two possibilities to loose. Either the angle of the pole is greater than 15Â° or the cart moves further away than 2.4 units from its origin. Basically, the algorithm will learn to prevent both conditions. To achieve this, here are two different sets of inputs, that could be used as an input for an AI:\n",
    "\n",
    "1. picture of the game\n",
    "2. angle and velocity of the pole !!TODO!! CHECK THIS INFORMATION\n",
    "\n",
    "This is our state representation, which tells the algorithm information about its surroundings. After each action we take, this information will be updated. The first case is the closest to the human perspektive. The algorithm just receives a flow of pictures and must return a useful actions to perform well at the game. The AI must find important features, this means the connection between input and affiliated action, by itself. For the second case, we, as a developer, already decided which features are useful. We already know, that the AI has enough information to decide on an action with just these two values. We will be using this input for the first example. This is done for the sake of simplicity, it should not be done in a real life use-case. Humans are naturally pretty bad at abstraction in comparison to a computer. We are also biased most of the time and this may prohibit us from recognizing useful features sometimes. An AI on the other hand will just look at the data and find the best patterns, but it needs the freedom to do so. In some cases this yields unexpected results, demonstrating strange dependencies between data. \n",
    "The set of action is just containing two movements: We can either move the cart left or right. Normally this would be done by pressing a button on a controller, but now our AI will do this for us.\n",
    "Finally, our score is the time, that our AI manages to balance the pole. Longer Times will result in higher scores. More information on this environment can be found at the [OpenAi Gym website](https://gym.openai.com/envs/CartPole-v1/).\n",
    "\n",
    "In this chapter, we took a look at our first environment and its rules. In the next step, we will build our first Reinforcement Learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make epochs viewable - required!<a name=\"epochvisible\"></a>\n",
    "\n",
    "For performance reasons, we do not want to have to wait and watch every training epoch our agent will complete. But to be honest, just looking at numbers is boring and not really helpful if our agent does not perform as expected. It is most interessting to watch some chosen epochs like the first, some between and finally the last. We can do this by collecting all frames, which are produced by the game and creating a gif afterwards. The idea for this solution was found at [[3]](https://stackoverflow.com/questions/40195740/how-to-run-openai-gym-render-over-a-server). The following code shows an example. First off, we will have to import two dependencies with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/jakevdp/JSAnimation.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import some things we need for this. The method \"display_frames_as_gif\" will create a gif for us out of a queue with all rendered images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of OpenAi Gym and rendering <a name=\"rendering\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed imports for gif creation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import os # for creating directories\n",
    "\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('CartPole-v0')\n",
    "\n",
    "environment = wrappers.Monitor(environment, 'modelOutput/test', video_callable=False ,force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateSize = environment.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionSize = environment.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDirectory = 'modelOutput/cartpole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outputDirectory):\n",
    "    os.makedirs(outputDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    \n",
    "    def __init__(self, stateSize, actionSize):\n",
    "        \n",
    "        self.stateSize = stateSize\n",
    "        self.actionSize = actionSize\n",
    "        \n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        \n",
    "        self.gamma = .95\n",
    "        \n",
    "        self.epsilon = 1.0 # 100% to exploration 0% to exploitation\n",
    "        self.epsilonDecay = .9965\n",
    "        self.epsilonMin = .001\n",
    "        \n",
    "        self.learningRate = .001\n",
    "        \n",
    "        self.model = self.buildModel()\n",
    "        \n",
    "        \n",
    "    def buildModel(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(24, input_dim = self.stateSize, activation = 'relu'))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(self.actionSize, activation = 'linear')) # directly, instead of propability or abstract\n",
    "        \n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learningRate))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def remember(self, state, action, reward, nextState, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, nextState, done))\n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.actionSize)\n",
    "        \n",
    "        actValue = self.model.predict(state)\n",
    "        \n",
    "        return np.argmax(actValue[0])\n",
    "    \n",
    "    \n",
    "    def replay(self, batchSize):\n",
    "        \n",
    "        miniBatch = random.sample(self.memory, batchSize)\n",
    "        \n",
    "        for state, action, reward, nextState, done in miniBatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(nextState)[0]))\n",
    "                \n",
    "            targetF = self.model.predict(state) # predicted future reward\n",
    "            targetF[0][action] = target\n",
    "            \n",
    "            self.model.fit(state, targetF, epochs = 1, verbose = 0)\n",
    "            \n",
    "        if self.epsilon > self.epsilonMin:\n",
    "            #print(\"Before: \" + str(self.epsilon))\n",
    "            self.epsilon *= self.epsilonDecay\n",
    "            #print(\"After: \" + str(self.epsilon))\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(stateSize, actionSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False;\n",
    "\n",
    "# buffer for rgb arrays to create a gif later on\n",
    "frames = []\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = environment.reset()\n",
    "    state = np.reshape(state, [1, stateSize])\n",
    "    \n",
    "    for time in range(5000):\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        nextState, reward, done, _ = environment.step(action)\n",
    "        \n",
    "        reward = reward if not done else -10\n",
    "        \n",
    "        nextState = np.reshape(nextState, [1, stateSize])\n",
    "        \n",
    "        agent.remember(state, action, reward, nextState, done)\n",
    "        \n",
    "        state = nextState\n",
    "        \n",
    "        if e > episodes-6:\n",
    "            frames.append(environment.render(mode = 'rgb_array'))\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e+1, episodes, time, agent.epsilon))\n",
    "            \n",
    "            if e > episodes-6:\n",
    "                display_frames_as_gif(frames)\n",
    "                frames.clear()\n",
    "            break\n",
    "        \n",
    "    if len(agent.memory) > batchSize:\n",
    "        agent.replay(batchSize)\n",
    "    \n",
    "    if e % 50 == 0:\n",
    "        agent.save(outputDirectory + \"weights \" + '{:04d}'.format(e) \n",
    "                   + \".hdf5\")\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook <a name=\"outlook\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources <a name=\"sources\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the following articles/blogposts/tutorials:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://gym.openai.com/ - information about OpenAi Gym\n",
    "\n",
    "[2] https://gym.openai.com/envs/CartPole-v1/ - information about the Cartpole environment of OpenAi Gym\n",
    "\n",
    "[3] https://stackoverflow.com/questions/40195740/how-to-run-openai-gym-render-over-a-server - solutions for rendering epochs on a headless server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
