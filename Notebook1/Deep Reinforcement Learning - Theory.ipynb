{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Beginners Tutorial (1) - Theory\n",
    "#### by Julian Bernhart, Robin Guth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Requirements](#requirements)\n",
    "1. [Targets](#targets)\n",
    "1. [Introduction](#introduction)\n",
    "    1. [Agents](#agent)\n",
    "1. [Reinforcement Learning](#rl)\n",
    "    1. [The concept of RL](#conceptRL)\n",
    "    1. [Q-Learning](#qlearning)\n",
    "1. [Outlook](#outlk)\n",
    "1. [Sources](#source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements: <a name=\"requirements\"></a>\n",
    "- Basic knowledge about Artifical Intelligence\n",
    "- Basic understanding of deep learning \n",
    "- Knowledge regarding neural networks (NN)\n",
    "- (Optional) Knowledge regarding convolutional neural networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targets <a name=\"targets\"></a>\n",
    "\n",
    "- introducing reinforcement learning\n",
    "- explaining components like agents, environments etc.\n",
    "- explaining Q-learning\n",
    "- explaining the use of deep learning in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "In this notebook the reader is presented with the theory behind RL. After the completion, we will have a basic understanding of reinforcement learning (RL) and its components (Agent, Environment, Action, ...) as well as knowledge about the theory for the implementation of RL in the second notebook.\n",
    "\n",
    "Deep Reinforcement Learning (also called RL) is a huge step towards the creation of an universal artificial intelligence. In 2013 a company, owned by Google, called ''Deep Mind'', was able to create an astonishing implementation of RL, which was capable to play retro games of the console ''Atari 2600''. In many cases the AI (Artificial Intelligence) was not only able to play the games successfully, but also exceeded human performances significantly. After these impressive results, it is definitly worth to take a closer look at Reinforcement Learning.\n",
    "\n",
    "Playing games is fun, but Reinforcement Learning has more to offer, as discussed in [[3]](#sources) . Apart from playing video games, there are use cases in many fields like robotics, traffic control or even personalized advertisement. While supervised and unsupervised learning are already used widely in production, reinforcement learning is still in development and further research is needed. \n",
    "As a fairly new topic, beginners often struggle to find a good starting point into the world of AI and specifically RL. Many tutorials are written for more advanced users, who already have a deeper unterstanding of machine learning. The ''Deep Reinforcement Learning Beginners Tutorial\" will provide an easy-to-follow, hands-on beginners guide to RL. After the completion, we will be able to write our own algorithm to play some basic games for us.\n",
    "\n",
    "Before we head into the world of Reinforcement Learning, we will have to talk about software agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents <a name=\"agent\"></a>\n",
    "\n",
    "The first component we will talk about are software agents. As described in [[1]](#sources), an agent is a program, which acts self sufficient to solve a task. There are three important points, that an agent should fulfill:\n",
    "\n",
    "- autonomous action : an agent needs to make decisions without external help\n",
    "- proactivity: an agent needs to execute actions to complete its task\n",
    "- reactiv: an agent needs to react on changes in its environment\n",
    "\n",
    "Finally there is one optional point:\n",
    "\n",
    "- ability to improve: an agent improves itself by building up knowledge after repetitively doing its task\n",
    "\n",
    "Basically, the part of the agent, which controls its actions, can be filled with different algorithms. In our case, this will be an implementation of RL, which fulfills all requirements of an agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning <a name=\"rl\"></a>\n",
    "\n",
    "Reinforcement Learning (RL) is considered one of the three machine learning paradigms, alongside supervised learning and unsupervised learning. The main goal of RL is to create an agent, which can navigate by actions in an environment in a way to maximize a representation of a cumulative reward. The evironment is a space that contains the agent. We will talk about the environment later on. In order to miximize the reward, the agent has to learn by trial and error which actions most likely lead to a reward and which lead to a penalty. After some training, the agent will use its knowledge to avoid previous mistakes. It still has to explore the environment, because otherwise we can not be sure if we actually find the global maximum or just a local one and if there might be a better chain of actions. If we think about our real world example in [[3]](#sources), this method of learning is pretty close the human learning. If we get hurt for example, we are more likely to try to avoid the situation. Still curiosity or necessity leads us to exploring and thus maybe getting us into danger, but in the best case, will lead us to some kind of positive reward.\n",
    "\n",
    "### The concept of RL <a name=\"conceptRL\"></a>\n",
    "\n",
    "Our agent is contained in an environment. A momentary snapshot of the environment is called state. A state contains all information at a certain point in time for example the position of the agent or enemies. There is a permanent exchange of information between the agent and its environment. The agent receives the actual state and has to choose an action based on its logic. Everything the agent can use to alter its environment is called an action. It changes the environment based on a certain set of rules. After executing an action the agents receives the new state and a reward, which helps to decide whether its decision leads to a positive result. Keep in mind, that a reward can also be negative. Basically, an environment is just a set of states. We can move between states by executing actions. A good example for an environment is our own world. We are agents, moving in the world. All the time we have to look at our surroundings and choose an action like moving accross the street or waiting until it is free. The laws of physics restrain us, take gravity for example. We receive rewards like geting hurt oder feeling satisfied, thats how we evalute our actions. Our environment is constantly changing, so we have to reevalute our decisions and choose an action again. The following picture shows the whole procedure: \n",
    "\n",
    "![Image of RLConcept](img/eoeSq.png \"RL concept diagram\")\n",
    "\n",
    "Source: https://i.stack.imgur.com/eoeSq.png\n",
    "\n",
    "Our goal is to learn the best action for each state, so that our total reward reaches its global maximum.  \n",
    "\n",
    "All components such as reward, amount and type of actions or states depend on the type of environment and can vary widely. As the real world example is pretty complex, for the rest of the notebook we will use a simple game environment for explanation. The informations available for an agent are the visual output of the game (actual state), the inputoptions and the reward for a previously taken action.  \n",
    "\n",
    "The following diagram shows an exemplary setup for a RL project with a game as an _environment_. For the navigation through this environment the _agent_ needs the actual _state_ (game screen) as an input, in order to respond with an appropriate _action_. Every action taken leads to a value added to the score of the agent and a change in the environment resulting in the next state of the game. \n",
    "\n",
    "In order to improve its knowledge, the agent has to repeat the task multiple times. For this we define a set amount of epochs with a set amount of time for the agent to play. \n",
    "\n",
    "This is where the RL part comes into play. In order to maxmize the reward, the agent has to learn which actions result in a positiv reward and should be prefered and which ones should be avoided because of a penalty. In the beginning, the agent has no knowledge about the environment and thus cannot make an educated desicion. It will need to explore by taking random actions and observing the reward. These informations will be added to the knowledge of the agent. Over time the agent needs to decrease the amount of explorational actions and increase the amount of exploitational action in order to progress. This means that the agent will start to decide on an action based on its gained knowledge, while decreasing the amount of random actions. A good decreasing ratio is very important for RL, as changing to fast will lead to bad results, while changing it to slow will lead to overfitting.\n",
    "\n",
    "![Image of RLSetup](img/mario.png)\n",
    "\n",
    "Source: https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419\n",
    "\n",
    "#### Summary:\n",
    "- Agent: accepts states, executes actions\n",
    "- Environment: the world in which the agent exists and operates\n",
    "- Action ($A_{t}$): a move the agent can make in the environment\n",
    "- State ($S_{t}$): a situation which the agent preceives, a representation of the environment\n",
    "- Observation ($S_{t+1}$): the state of the environment after taking actions \n",
    "- Reward($R_{t}$): feedback that measures the success or failure of the agent's actions\n",
    "- Future Reward($R_{t+1}$): reward after taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Aspect\n",
    "\n",
    "The agent has to recognize and differentiate elements in the environment, to be able to act accordingly and choose a proper action. In the best case, we already have the input reduced into some important numbers, which describe the actual state. In real applications this may not be possible or feasible with our knowledge. Most of the time we will have a picture available for example the image of a camera or a game screen, but the input we receive can basically be any sensory data.\n",
    "\n",
    "Furthermore the agent must associate the occurence of individual elements on screen with its own actions and the subsequent reward or penalty, which may occur only after several steps in the future. Depending on the situation it may be also necessary to estimate what the next state of the game could be. There are two possible ways to use the Deep Learning aspect.\n",
    "\n",
    "1. Image processing with a Convolutional Neural Network (CNN), in order to process the image information and send this to the agent.\n",
    "2. A Deep Q-Learning Network (DQN), in order to model the Q-function, which is discussed below.\n",
    "\n",
    "Following is an exemplary presentation of a neural network, which is designed according to the tasks just mentioned.\n",
    "\n",
    "![Image of ExampleDQN](img/v2-67ef75bb7f5e67b2a42645aa821894bf_hd.png \"Exemplary DQN\")\n",
    "\n",
    "Source: https://zhuanlan.zhihu.com/p/25239682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to learn in deep reinforcement learning. Eiher we can use _Value Learning_ or _Policy Learning_. With  _Value Learning_, we assign values to each state-action pair, which correspond to the probabilities of the pair beeing the best option. In _Policy Learning_, we will learn a strategy instead, which gives as the best estimated action for a given state s. In this notebook we will have a look at  _Value Learning_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning <a name=\"qlearning\"></a>\n",
    "\n",
    "The task of the agent is the maximization of a cumulative future reward. An example of this can be the score in a game. In an environment, there is no guarantee for an immediate reward after executing an action. In many cases, only specific chains of actions lead to a positive reward, so the agent needs to learn multiple moves in succession in order to fulfill the given task. For example, if the agent needs to collect an object in order to get a higher score, it may be necessary to take multiple moves to reach it. The moves in between may not increase the reward significantly or even reduce it, however they are needed to complete the task in order to be successfull.\n",
    "\n",
    "A simple way to implement RL is Q-Learning. We assign a numerical value to each action we can execute per state. This value is called Q-value. It represents an estimate of which action will result in the highest reward for the actual state. This is the knowledge of our agent. Each step we have to choose an action for the actual state. This is either random or knowledge based. We use random actions to explore our environment. If we use the knowledge of the agent, the next action is chosen by exploitation, so the agent uses the action with the highest Q-value for the given state. \n",
    "\n",
    "The Q-Values are not known at the beginning and need to be learned by training. For this purpose, the agent needs to explore its environment. In the best case, it will visit all states through every possible action, but most of the time, this is neither needed nor possible, because of the amount of states or actions. We only desire the path with the best overall reward. We have to keep in mind, that there is no guarantee, that a path we found is the global maximum. It is possible, that the agent is stuck at a local maximum and needs to sacrifice some reward to be able to reach a state with a bigger payout. The Q-values are initialized by $0$. After every step the agent takes, we need to update the last used Q-value according to the reward the agent receives. If the agent loses, dies or does something else we want to avoid, the resulting reward is negative. Either positive or negative, the Q-value needs to reflect this with its value. We need to update the Q-value regardless if the agent is exploring or exploiting. The exploration rate $\\epsilon$ is slowly descending while training, so the agent can use its gained knowledge to improve the reward.\n",
    "\n",
    "In order to adjust the Q-values we need the _total reward_:\n",
    "\n",
    ">$R_t = \\sum\\limits_{i=t}^{\\infty} r_i = r_t + r_{t+1} + ... + r_{t+n} + ...$\n",
    "\n",
    "The _total reward_ $R_t$ is the sum of the actual reward and all future rewards from  actual state _t_ to state  n = $\\infty$.\n",
    "\n",
    "We also add a discount factor $\\gamma$ to this formula. That factor has a range of value between/from $0$ to $1$ and is used to decrease rewards that are further into the future. This makes rewards in near future more dicisive for the agent.  \n",
    "\n",
    ">$R_t = \\sum\\limits_{i=t}^{\\infty} \\gamma _i r_i = \\gamma _{t} r_t + \\gamma _{t+1} r_{t+1} + ... + \\gamma _{t+n} r_{t+n} + ...$\n",
    "\n",
    "The total reward $R_t$ is the discounted sum of future rewards. \n",
    "\n",
    "\n",
    "#### Q-function\n",
    "\n",
    "The Q-function is the name giving aspect of a Deep Q-Learning Network (DQN), while the \"Q\" stands for \"Quality\". The higher the reward, the higher the estimated Q-value and associated quality. This function represents the expected total future reward an agent could reach by executing a certain action $a_t$ in a given state $s_t$ and is used for our deep neural network. The DQN takes over the task of estimating this Q-value. With no prior information at the beginning, the agent needs to extends its knowledge by trial and error and thus learning, how to determine a Q-value out of the actual state and an action.\n",
    "\n",
    ">$<$Q-Value depending on $s$ and $a$ $> = <$Expected total future reward$>$\n",
    "<br>\n",
    "<br>\n",
    "$Q(s, a) = E[R_t]$\n",
    "\n",
    "If we have a good approximation of the Q-values for each state-action-pair after the exploration, the agent needs to decide on which action is the best for a given state. This means the agents needs to choose the action which maximizes the estimated future reward by the associated Q-value, with the help of a policy $\\pi^*(s)$. This is the part of exploiting the gained knowledge.\n",
    "\n",
    ">$<$Optimal policy depending on $s$$> = <$Action resulting in the highest future reward according to current knowledge$>$\n",
    "<br>\n",
    "<br>\n",
    "$\\pi^*(s) = {\\underset{a}{\\operatorname{argmax}}Q(s,a)}$\n",
    "\n",
    "Finally, we will take a look at the formula, which we will actually use to calculate Q-values. At the moment we use the current state of the environment, the actions to take and the reward, resulting from the decision of the agent as input. We also use the following state of the environment. We introduce the _Bellman equation_ now:\n",
    "\n",
    ">$<$Q-value depending on $s_t$ and $a_t$ $> = <$ Reward observed after the last action (immediate reward)$> + <$ Discount factor $> * <$ Maximum Q-value for the next state of the game (future reward)$>$\n",
    "<br>\n",
    "<br>\n",
    "$Q(s_t, a_t) = r_t + \\gamma * \\max\\limits_{a'} Q(s', a')$\n",
    "\n",
    "For now the Bellman equation provides the Q-value for a given action in a given state. In order to train our DQN regarding the Q-function, it is necessary to compare the Q-values predicted with the actual ones and minimize the error between the two. If we use the Bellman equation to calculate this difference, by simply subtracting these values, we get the so called _temporal difference error_.\n",
    "\n",
    ">$<$Temporal difference error$> = <$ Target Q-value$> - <$ Predicted Q-value$>$\n",
    "<br>\n",
    "<br>\n",
    "$\\delta = ( r_t + \\gamma * \\max\\limits_{a'} Q(s', a')) - Q(s_t, a_t)$\n",
    "\n",
    "To update an old Q-value, we use the temporal difference error and add it to the Q-value. A new discount factor, called learning rate $\\alpha$, determines the weight of the temporal difference error.\n",
    "\n",
    ">$<$new Q-value depending on $s_t$ and $a_t$ $> = <$old Q-value depending on $s_t$ and $a_t$ $> + <$Temporal difference error$>$\n",
    "<br>\n",
    "<br>\n",
    "$Q(s_t, a_t)_{new} = Q(s_t, a_t)_{old} + \\alpha * \\delta$\n",
    "\n",
    "Now we use _gradient descent_, an optimization algorithm, to optimize the Q-value approximation. Therefor we can train the DQN using _mean square error_ as the loss function, which results in the the following formula.\n",
    "\n",
    ">$L = E[\\lVert \\delta \\rVert ^2]$\n",
    "\n",
    "#### Model Q-function <a name=\"modellq\"></a>\n",
    "\n",
    "The following illustration shows two options by which we can model our Q-function with a deep neural network. The model on the left side estimates one Q-value at the time. This is what we are using at the moment. The model on the right side is the prefered one, because it increases the efficiency. It gets the state of the environment and calculates multiple Q-values, so we just have to choose the highest value to determine the action for the agent instead of estimating the Q-values for every action individually. This will save processing power, as the amount of possible actions can be very large.\n",
    "\n",
    "![Image of DQN](img/main-qimg-0773775bf325ecc02d9b2f8374e2edaa.png \"DQN models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook <a name=\"outlk\"></a>\n",
    "\n",
    "In this notebook, we took a look at the basics of Reinforcement Learning. We talked about agents, environments and the relationship between the two. We introduced RL as a way to control an agent and explained two different approaches to implement RL with Q-learning. We took a deeper look at the Q-learning function and other components of an RL agent.\n",
    "\n",
    "We can now continue with the second notebook, there we will create our first agent with Python. \n",
    "\n",
    "As RL is a very complex topic, we recommend the reader to take a look at the sources or some of the following follow-up links for more in-depth information:\n",
    "\n",
    "https://www.youtube.com/watch?v=2pWv7GOvuf0 - full lecture series on RL from Google Deep Mind.\n",
    "\n",
    "https://www.youtube.com/watch?v=i6Mi2_QM3rA - full lecture on RL from MIT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources <a name=\"source\"></a>\n",
    "\n",
    "This notebook is based on the following tutorial:\n",
    "\n",
    "https://www.youtube.com/watch?v=i6Mi2_QM3rA - theory about DRL\n",
    "\n",
    "Further information was takenfrom the following articles/blogposts/tutorials:\n",
    "\n",
    "[1] https://wirtschaftslexikon.gabler.de/definition/agent-28615/version-252241 - definition of a software agent (german)\n",
    "\n",
    "[2] Playing Atari with Deep Reinforcement Learning; Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin A. Riedmiller; 2013 - reinforcement learning with atari games\n",
    "\n",
    "[3] https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12 - use cases for reinforcement learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
