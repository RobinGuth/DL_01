{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Beginners Tutorial \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAi Gym\n",
    "\n",
    "As we discussed before, Reinforcement Learning can be used to solve a range of different problems. Developing Machine Learning algorithms is often not easy to understand nor comprehensible especially for beginners. Furthermore, it is important to be able to compare the performance of different iterations of our algorithm, to be able to improve it. \n",
    "\n",
    "So bascially we need an environment, that we can use to test and train our RL agent, which fulfills the following requirements:\n",
    "\n",
    "- repeatable test/training epochs\n",
    "- finite set of inputs\n",
    "- finite set of actions\n",
    "- easy state representation\n",
    "- easy to control agent\n",
    "- deliver a score for a given state\n",
    "- !!TODO!! INSERT OTHER REQUIREMENTS HERE\n",
    "\n",
    "In practice, not all of these points will be fulfilled, but as this is a beginners guide, we will start with a simple environment. Luckily, many video games can be used as quite good environments for machine learning purposes.  Many implementations of RL are tested with games as Benchmark. \n",
    "\n",
    "!!TODO!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step in the creation of an AI, we should always look at our environment, to better understand what we want to achieve with the algorithm. The game, which will be used as environment, is called Cartpole. It involes a pretty simple task: The player tries to balance a pole in a 2D world without letting it tip over. We can play this game in the real world with something like a broomstick. This may seem trivial at first, but this task gets much harder if the pole is short. If we try the same with a pen for example, we will likely fail to balance it for a longer period of time. In the game, difficulty is reached by making the pole very sensitive to not beeing perpendicular to the ground and accelerating very fast. Instead of our hand, the pole is resting on a small cart, that the player can move right or left. Our RL algorithm will replace the player completely and will have to do all tasks a human player would need to do. An image of Cartpole is shown below.\n",
    "\n",
    "![Image of Cartpole](img/cartpole01.PNG \"Exemplary Cartpole\")\n",
    "\n",
    "Cartpole is an endless game and there are only two possibilities to loose. Either the angle of the pole is greater than 15Â° or the cart moves further away than 2.4 units from its origin. Basically, the algorithm will learn to prevent both conditions. To achieve this, here are two different sets of inputs, that could be used as an input for an AI:\n",
    "\n",
    "1. picture of the game\n",
    "2. angle and velocity of the pole !!TODO!! CHECK THIS INFORMATION\n",
    "\n",
    "This is our state representation, which tells the algorithm information about its surroundings. After each action we take, this information will be updated. The first case is the closest to the human perspektive. The algorithm just receives a flow of pictures and must return a useful actions to perform well at the game. The AI must find important features, this means the connection between input and affiliated action, by itself. For the second case, we, as a developer, already decided which features are useful. We already know, that the AI has enough information to decide on an action with just these two values. We will be using this input for the first example. This is done for the sake of simplicity, it should not be done in a real life use-case. Humans are naturally pretty bad at abstraction in comparison to a computer. We are also biased most of the time and this may prohibit us from recognizing useful features sometimes. An AI on the other hand will just look at the data and find the best patterns, but it needs the freedom to do so. In some cases this yields unexpected results, demonstrating strange dependencies between data. \n",
    "The set of action is just containing two movements: We can either move the cart left or right. Normally this would be done by pressing a button on a controller, but now our AI will do this for us.\n",
    "Finally, our score is the time, that our AI manages to balance the pole. Longer Times will result in higher scores.\n",
    "\n",
    "In this chapter, we took a look at our first environment and its rules. In the next step, we will build our first Reinforcement Learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install & import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install some dependencies to render epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/jakevdp/JSAnimation.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import os # for creating directories\n",
    "\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#needed for gif\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('CartPole-v0')\n",
    "\n",
    "environment = wrappers.Monitor(environment, 'modelOutput/test', video_callable=False ,force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateSize = environment.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionSize = environment.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDirectory = 'modelOutput/cartpole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outputDirectory):\n",
    "    os.makedirs(outputDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Gif Making Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    \n",
    "    def __init__(self, stateSize, actionSize):\n",
    "        \n",
    "        self.stateSize = stateSize\n",
    "        self.actionSize = actionSize\n",
    "        \n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        \n",
    "        self.gamma = .95\n",
    "        \n",
    "        self.epsilon = 1.0 # 100% to exploration 0% to exploitation\n",
    "        self.epsilonDecay = .9965\n",
    "        self.epsilonMin = .001\n",
    "        \n",
    "        self.learningRate = .001\n",
    "        \n",
    "        self.model = self.buildModel()\n",
    "        \n",
    "        \n",
    "    def buildModel(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(24, input_dim = self.stateSize, activation = 'relu'))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(self.actionSize, activation = 'linear')) # directly, instead of propability or abstract\n",
    "        \n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learningRate))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def remember(self, state, action, reward, nextState, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, nextState, done))\n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.actionSize)\n",
    "        \n",
    "        actValue = self.model.predict(state)\n",
    "        \n",
    "        return np.argmax(actValue[0])\n",
    "    \n",
    "    \n",
    "    def replay(self, batchSize):\n",
    "        \n",
    "        miniBatch = random.sample(self.memory, batchSize)\n",
    "        \n",
    "        for state, action, reward, nextState, done in miniBatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(nextState)[0]))\n",
    "                \n",
    "            targetF = self.model.predict(state) # predicted future reward\n",
    "            targetF[0][action] = target\n",
    "            \n",
    "            self.model.fit(state, targetF, epochs = 1, verbose = 0)\n",
    "            \n",
    "        if self.epsilon > self.epsilonMin:\n",
    "            #print(\"Before: \" + str(self.epsilon))\n",
    "            self.epsilon *= self.epsilonDecay\n",
    "            #print(\"After: \" + str(self.epsilon))\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(stateSize, actionSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False;\n",
    "\n",
    "# buffer for rgb arrays to create a gif later on\n",
    "frames = []\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = environment.reset()\n",
    "    state = np.reshape(state, [1, stateSize])\n",
    "    \n",
    "    for time in range(5000):\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        nextState, reward, done, _ = environment.step(action)\n",
    "        \n",
    "        reward = reward if not done else -10\n",
    "        \n",
    "        nextState = np.reshape(nextState, [1, stateSize])\n",
    "        \n",
    "        agent.remember(state, action, reward, nextState, done)\n",
    "        \n",
    "        state = nextState\n",
    "        \n",
    "        if e > episodes-6:\n",
    "            frames.append(environment.render(mode = 'rgb_array'))\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e+1, episodes, time, agent.epsilon))\n",
    "            \n",
    "            if e > episodes-6:\n",
    "                display_frames_as_gif(frames)\n",
    "                frames.clear()\n",
    "            break\n",
    "        \n",
    "    if len(agent.memory) > batchSize:\n",
    "        agent.replay(batchSize)\n",
    "    \n",
    "    if e % 50 == 0:\n",
    "        agent.save(outputDirectory + \"weights \" + '{:04d}'.format(e) \n",
    "                   + \".hdf5\")\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the following articles/blogposts/tutorials:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://gym.openai.com/envs/CartPole-v1/ - information about the Cartpole environment of OpenAi Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
