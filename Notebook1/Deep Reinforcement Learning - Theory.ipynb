{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Beginners Tutorial - Theory\n",
    "\n",
    "Deep Reinforcement Learning (also called RL) is a huge step towards the creation of an universal artificial intelligence. In 2013 a company, owned by Google, called ''Deep Mind'', was able to create an astonishing implementation of RL, which was capable to play retro games of the console ''Atari 2600''. In many cases the AI (Artificial Intelligence) was not only able to play the games successfully, but also exceeded human performances significantly. After the impressive results, it is definitly worth to take a closer look at Reinforcement Learning.\n",
    "\n",
    "While playing games is fun, as discussed in [[3]](#sources) Reinforcement Learning has more to offer. Apart from playing video games, there are use cases in fields like robotics, traffic control or even personalized advertisement. While supervised/unsupervised learning is already used in production, reinforcement learning is still in development and further research is needed. \n",
    "As a fairly new topic, beginners often struggle to find a good starting point into the world of AI and specifically RL. Many tutorials are written for more advanced users, who already know basics of machine learning. The ''Deep Reinforcement Learning Beginners Tutorial\" will provide an easy-to-follow, hands-on beginners guide to RL. After the completion, we will be able to write our own algorithm to play some basic games for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Requirements](#requirements)\n",
    "1. [Targets](#target)\n",
    "1. [Introduction](#introduction)\n",
    "1. [Outlook](#outlk)\n",
    "1. [Sources](#source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements: <a name=\"requirements\"></a>\n",
    "- Basic knowledge about Artifical Intelligence\n",
    "- Basic understanding of deep learning \n",
    "- Knowledge regarding neural networks (NN)\n",
    "- (Optional) Knowledge regarding convolutional neural networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets <a name=\"target\"></a>\n",
    "- Basic understanding of reinforcement learning (RL) and its components (Agent, Environment, Action, ...)\n",
    "- Providing the necessary theory for the implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIND GOOD HEADLINE\n",
    "Before we head into the world of Reinforcement Learning, we will have to talk about some basic concepts.\n",
    "### Agents <a name=\"agents\"></a>\n",
    "\n",
    "The first component we will talk about are software agents. As described in [[1]](#sources), an agent is a programm, which acts self sufficient to solve a task. There are three important points, that an agent should fulfill:\n",
    "\n",
    "- autonomous action : an agent needs to make decisions without external help\n",
    "- proactivity: an agent needs to needs to execute actions to complete its task\n",
    "- reactiv: an agent needs to react on changes in its environment\n",
    "\n",
    "Finally there is one optional point:\n",
    "\n",
    "- ability to improve: an agent improves itself by building up knowledge after repetitively doing its task\n",
    "\n",
    "Basically, the part of the agent, which controls its actions, can be filled with different algorithms. In our case, this will be an implementation of RL.\n",
    "\n",
    "### Environment\n",
    "\n",
    "The environment contains our agent. A momentary snapshot of the environment is called state. A state contains all information in a certain time for example the position of the agent or enemies. There is a permanent circle between the agent and its environment. The agent receives the actual state and has to choose an action. Everything the agent can use to alter its environment is called action. It changes the environment based on a certain set of rules and the agent receives the new state. After executing an action the agents receives a reward, which helps it to decide whether his decision was good. Keep in mind, that a reward can also be negative. An accurate example for an evironment is our own world. You are an agent, moving in the world. All the time you have to look at your surroundings and choose an action like moving accross the street. The laws of physics restrain you, for example gravity. You receive rewards like geting hurt oder feeling satisfied, thats how we evalute our actions. Our environment is constantly changing, so we have to reevalute our decisions and choose an action again. The following picture shows the whole procedure: \n",
    "\n",
    "![Image of RLConcept](img/eoeSq.png \"RL concept diagram\")\n",
    "\n",
    "Source: https://i.stack.imgur.com/eoeSq.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Reinforcement Learning<a name=\"introduction\"></a>\n",
    "\n",
    "Reinforcement Learning (RL) is considered one of the three machine learning paradigms, alongside supervised learning and unsupervised learning. The main goal of RL is to create an agent, which can navigate by actions in an environment in a way to maximize a representation of a cumulated reward. In order to miximize the reward, the agent has to learn by trial and error which actions most likely result in a future reward and which lead to a penalty. After some training, the agent will use its knowledge to avoid previous mistakes. It still has to explore the environment, because otherwise we can not be sure if we actually find the global maximum or just a local one and if there might be a better chain of actions. If we think about our real world example, this method of learning is pretty close the human learning. If we get hurt for example, we are more likely to try to avoid the situation. Still curiosity sometimes leads us to exploring and thus maybe getting us into danger. \n",
    "\n",
    "As the real world example is pretty complex, for the rest of the notebook we will use a game environment to explain things. The informations available for an agent are the visual output of the game (actual state), the inputoptions and the reward for a previously taken action.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The concept of RL\n",
    "\n",
    "The following diagramm shows an exemplary setup for a RL project with a game as an _environment_. For the navigation through this environment the _agent_ needs the actual game screen (_state_) as an input, in order to respond with a corresponding _action_. Every action taken leads to a value added to the score of the agent and a change in the environment resulting in the next state of the game. \n",
    "\n",
    "In order to improve its knowledge, the agent has to play the game multiple times. For this we define a set amount of epochs with a set amount of time for the agent to play. \n",
    "\n",
    "This is where the RL part comes into play. In order to maxmize the reward, the agent has to learn which actions result in a positiv reward and should be prefered and which ones should be avoided because of a negativ reward. In the beginning, the agent has no knowledge about the environment and thus cannot make an educated desicion. It will need to explore by taking random actions and observing the reward. Over time the agent needs to decrease the amount of explorational actions and increase the amount of exploitational action in order to progress. This means that the agent will start to decide on an action based on its gained knowledge, while decreasing random actions. A good decreasing ratio is a k\n",
    "\n",
    "![Image of RLSetup](1-aKYFRoEmmKkybqJOvLt2JQ.png \"RL setup diagram\")\n",
    "Source: https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419\n",
    "\n",
    "## Summary:\n",
    "- Agent: accepts states, executes actions\n",
    "- Environment: the world in which the agent exists and operates\n",
    "- Action ($A_{t}$): a move the agent can make in the environment\n",
    "- State ($S_{t}$): a situation which the agent preceives, a representation of the actual picture of the environment\n",
    "- Observation ($S_{t+1}$): the state of the environment after taking actions \n",
    "- Reward($R_{t}$): feedback that measures the success or failure of the agent's actions\n",
    "- Future Reward($R_{t+1}$): reward after taking actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Aspect\n",
    "\n",
    "The agent has to recognize and differentiate game elements in a frame of the game. Furthermore it must associate the occurence of individual elements on screen with its own actions and the subsequent reward or penalty, which may occur only after several steps in the future. Depending on the situation it may be also necessary to estimate what the next state of the game could be. There are two possible ways to use the Deep Learning aspect.\n",
    "\n",
    "1. Image processing with a Convolutional Neural Network (CNN), in order to process the image information.\n",
    "2. A Deep Q-Learning Network (DQN), in order to model the Q-function, which is discussed below.\n",
    "\n",
    "Following is an exemplary presentation of a neural network, which is designed according to the tasks just mentioned.\n",
    "\n",
    "![Image of ExampleDQN](img/v2-67ef75bb7f5e67b2a42645aa821894bf_hd.png \"Exemplary DQN\")\n",
    "\n",
    "Source: https://zhuanlan.zhihu.com/p/25239682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "The task of the agent is the maximization of a cumulative future reward. An example of this could be the score in a game. In a game not every single move leads to an immediate reward, so the agent needs to learn multiple moves in succession in order to fulfill the given task. For example, if the agent needs to collect an object in order to get a highter score, it may be necessary to take multiple moves to reach it. So the moves in between doesn't increase the reward significantly, however they are needed to complete the task. \n",
    "\n",
    "The easiest variant to realize RL is Q-Learning. Therefor, so-called Q-Values are estimated. These assign a state of the game environment to a numerical value for every possible action of the agent. The Q-Value then provides an estimate of which action is most likely to result in the highest future reward. This will select the appropriate action for the agent.\n",
    "\n",
    "The Q-Values are not known at the beginning and have to be estimated somehow. For this purpose, these are initialized to zero and repeatedly updated as soon as the agent collects new information. The Q-Value must be updated every time the agent scores points. If the agent loses or dies, the resulting reward is $0$.\n",
    "\n",
    "In order to do that we need the _total reward_:\n",
    "\n",
    ">$R_t = \\sum\\limits_{i=t}^{\\infty} r_i = r_t + r_{t+1} + ... + r_{t+n} + ...$\n",
    "\n",
    "We also add a discount factor $\\gamma$ to this formula. That factor has a range of value between/from $1$ to $0$ and decreases the further the reward lays in the future, this way the rewards in near future are more dicisive for the agent.  \n",
    "\n",
    ">$R_t = \\sum\\limits_{i=t}^{\\infty} \\gamma _i r_i = \\gamma _{t} r_t + \\gamma _{t+1} r_{t+1} + ... + \\gamma _{t+n} r_{t+n} + ...$\n",
    "\n",
    "The total reward $R_t$ is the discounted sum of all rewards obtained from time t. \n",
    "\n",
    "\n",
    "## Q-function\n",
    "\n",
    "The Q-function is the name giving aspect of a Deep Q-Learning Network, while the \"Q\" stands for \"Quality\". The higher the reward, the higher the estimated Q-value and associated quality. This function captures the expected total future reward an agent could reach by executing a certain action $a_t$ in a given state $s_t$ and is used for our deep neural network. The DQN takes over the task of estimating this Q-value. At the beginning the agent has to gain knowledge by exploring the environment. To achieve this, observing random actions deliver the information needed. \n",
    "\n",
    ">$<$Q-Value depending on $s$ and $a$ $> = <$Expected total future reward$>$\n",
    "<br>\n",
    "<br>\n",
    "$Q(s, a) = E[R_t]$\n",
    "\n",
    "If we have a good approximation of the Q-function for every state-action-pair, the agent needs to decide what's the best action to take in the given state. This means the agents needs to choose the action which maximizes the future Reward by the associated Q-value, with the help of a policy $\\pi^*(s)$. This is the part of exploiting the gained knowledge.\n",
    "\n",
    ">$<$Optimal policy depending on $s$$> = <$Action resulting in the highest future reward according to current knowledge$>$\n",
    "<br>\n",
    "<br>\n",
    "$\\pi^*(s) = {\\underset{a}{\\operatorname{argmax}}Q(s,a)}$\n",
    "\n",
    "At the moment we use the current state of the game, the actions to take and the reward, resulting from the decision of the agent. In order to improve the agents performance, we can also add the following state of the game. This is where the _Bellman equation_ comes in handy.\n",
    "\n",
    "The solution mentioned above looks in formula like this:\n",
    "\n",
    ">$<$Q-value depending on $s_t$ and $a_t$ $> = <$ Reward observed after the last action (immediate reward)$> + <$ Discount factor $> * <$ Maximum Q-value for the next state of the game (future reward)$>$\n",
    "<br>\n",
    "<br>\n",
    "$Q(s_t, a_t) = r_t + \\gamma * \\max\\limits_{a'} Q(s', a')$\n",
    "\n",
    "For now the Bellman equation provides the Q-value for a given action in a given state. In order to train our DQN regarding the Q-function, it is necessary to compare the Q-values predicted with the actual ones and minimize the error between the two. If we use the Bellman equation to calculate this difference, by simply subtracting these values, we get the so called _temporal difference error_.\n",
    "\n",
    ">$<$Temporal difference error$> = <$ Target Q-value$> - <$ Predicted Q-value$>$\n",
    "<br>\n",
    "<br>\n",
    "$\\delta = ( r_t + \\gamma * \\max\\limits_{a'} Q(s', a')) - Q(s_t, a_t)$\n",
    "\n",
    "Now we use _gradient descent_, an optimization algorithm, to optimize the Q-value approximation. Therefor we can train the DQN using _mean square error_ as the loss function, which results in the the following formula.\n",
    "\n",
    ">$L = E[\\lVert \\delta \\rVert ^2]$\n",
    "\n",
    "## Model Q-functions\n",
    "\n",
    "The following illustration shows two options by which we can model our Q-function with a deep neural network. The model on the left side estimates one Q-value at the time. The model on the right side is the prefered one, because of the increase in efficiency. It gets the state of the game and calculates multiple Q-values, after we get the result we have to choose the highest value to determine the action for the agent.\n",
    "\n",
    "![Image of DQN](img/main-qimg-0773775bf325ecc02d9b2f8374e2edaa.png \"DQN models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook <a name=\"outlk\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources <a name=\"source\"></a>\n",
    "\n",
    "This notebook is based on the following tutorial:\n",
    "\n",
    "https://www.youtube.com/watch?v=i6Mi2_QM3rA - theory about DRL\n",
    "\n",
    "Further information was takenfrom the following articles/blogposts/tutorials:\n",
    "\n",
    "[1] https://wirtschaftslexikon.gabler.de/definition/agent-28615/version-252241 - definition of a software agent (german)\n",
    "\n",
    "[2] Playing Atari with Deep Reinforcement Learning; Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin A. Riedmiller; 2013 - reinforcement learning with atari games\n",
    "\n",
    "[3] https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12 - use cases for reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
