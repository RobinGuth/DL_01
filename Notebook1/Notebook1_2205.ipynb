{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__fett__\n",
    "<br>\n",
    "_kursiv_\n",
    ">indent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit:\n",
    "\n",
    "### Requirements:\n",
    "- Basic understanding of reinforcement learning (RL)\n",
    "- Basic understanding of deep learning in terms of RL\n",
    "- keras\n",
    "- numpy\n",
    "...\n",
    "\n",
    "### Target\n",
    "- Understanding {Agent, Environment, Action}\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement learning (RL) is considered one of the three machine learning paradigms, alongside supervised learning and unsupervised learning. The task of the topic of RL is to navigate an agent by actions in an environment in a way to maximize a representation of a cummulativ reward. In order to miximize that reward the agent has to learn by trial and error which actions most likely result in a future reward and which lead to a penalty. An example of an environment is an game environment. The informations available for agent are the visual output of the game, the inputoptions and whether a previous action results in a reward or a penalty.  \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "- Warum ein Spiel als Beispiel? \n",
    "\n",
    "- Was muss Agent leisten?\n",
    "    - Muss einzelne Spielelemente erkennen und unterscheiden\n",
    "    - Muss Auftreten einzelner Elemente auf dem Bildschirm mit den eigenen Aktionen und dem darauf folgenden Reward/Penalty in Verbindung bringen, welche auch erst nach mehreren Schritten in der Zukunft liegen können.\n",
    "\n",
    "<br>\n",
    "- In diesem Notebook: Wie beurteilt Agent Reward etc. (Q-Value), wenn er ein Bild der Spielumgebung sieht. Q-Value zeigt Agent, welche Aktion am wahrscheinlichsten zu dem höchsten kummulativen Reward (in der Zukunft) führt. Das Problem wird auf das finden des höchsten Q-Values reduziert -> Agent angepasst.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "Daten: state-action pairs\n",
    "Ziel: Maximize future rewards over many tme steps\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "exploration vs exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent, Spielumgebung etc. erklären"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "total reward -> future reward ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagramm shows the exemplary setup for a RL project with \"Super Mario Bros\" as an _environment_. For the navigation through this environment the _agent_ needs the actual game screen (_state_) as an input in order to respond with a corresponding _action_. Every action taken leads to a value added to the score of the agent and a change in the environment resulting in the next state of the game. \n",
    "\n",
    "In order to improve the agent has to play the game multiple times. For this we define a set amount of epochs with a set amount of time for the agent to play. \n",
    "\n",
    "This is where the RL part comes into play. In order to maxmize the reward, the agent has to learn which actions result in a positiv reward and should be prefered and which ones should be avoided because of a negativ reward. Because of missing knowledge the agent is not able to decide which action should be the best to take in the given state, so it has to explore the environment by taking random actions and observing the reward. Over time the agent needs to decrease the amount of explorational actions and increase the amount of exploitational action in order to progress. This means that the random actions became fewer while the use of learned knowledge becames more often. \n",
    "\n",
    "deep learning\n",
    "action -> change state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The concept of RL\n",
    "\n",
    "\n",
    "\n",
    "![Image of RLSetup](img/1-aKYFRoEmmKkybqJOvLt2JQ.png \"RL setup diagram\")\n",
    "Source: https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419\n",
    "\n",
    "### Summary:\n",
    "- Agent: takes actions\n",
    "- Environment: the world in which the agent exists and operates\n",
    "- Action ($A_{t}$): a move the agent can make in the environment\n",
    "- State ($S_{t}$): a situation which the agent preceives, a representation of the actual picture of the environment\n",
    "- Observation ($S_{t+1}$): the state of the environment after taking actions \n",
    "- Reward($R_{t}$): feedback that measures the success or failure of the agent's actions\n",
    "- Future Reward($R_{t+1}$): \n",
    "\n",
    "![Image of RLConcept](img/eoeSq.png \"RL concept diagram\")\n",
    "\n",
    "Source: https://i.stack.imgur.com/eoeSq.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "The task of the agent is the maximization of a cumulative future reward. An example of this could be the score in a game. In a game not every single move leads to an immediate reward, so the agent needs to learn multiple moves in succession in order to fulfill the given task. For example, if the agent needs to collect an object in order to get a highter score, it may be necessary to take multiple moves to reach it. So the moves in between doesn't increase the reward significantly, however they are needed to complete the task. \n",
    "\n",
    "In order to do that we need the _total reward_:\n",
    "\n",
    "$R_t = \\sum\\limits_{i=t}^{\\infty} r_i = r_t + r_{t+1} + ... + r_{t+n} + ...$\n",
    "\n",
    "We also add a discount factor $\\gamma$ to this formula. That factor has a range of value between/from $1$ to $0$ and decreases the further the reward lays in the future, this way the rewards in near future are more dicisive for the agent.  \n",
    "\n",
    "$R_t = \\sum\\limits_{i=t}^{\\infty} \\gamma _i r_i = \\gamma _{t} r_t + \\gamma _{t+1} r_{t+1} + ... + \\gamma _{t+n} r_{t+n} + ...$\n",
    "\n",
    "The total reward $R_t$ is the discounted sum of all rewards obtained from time t. \n",
    "\n",
    "### Q-function\n",
    "Rt dicounted summ of all rewards obtained from time t\n",
    "Q-function captures the expected total future reward (DQN)\n",
    "estemating Q-Value -> DQN\n",
    "\n",
    "The Q-function is the name giving aspect of a Deep Q-Learning Network. This function captures the expected total future reward an agent could reach by executing a certain action $a_t$ in a given state $s_t$ and is used for our deep neural network. \n",
    "\n",
    "$Q(s_t, a_t) = r_t + \\gamma * \\max\\limits_{a} Q(s_{t+1}, a)$\n",
    "\n",
    "Following we have two options shown, how to use a deep neural network to model the Q-function.\n",
    "\n",
    "\n",
    "### Value Learning\n",
    "\n",
    "### Policy Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem (warum Q-Learning?)\n",
    "\n",
    "- Mensch: \n",
    "    - herausfinden, was was macht? (Controls etc.) \n",
    "    - Ziel herausfinden\n",
    "    - herausfinden und lernen von negativen Einflüssen und vermeiden\n",
    "    \n",
    "## Q-Learning\n",
    "\n",
    "vergehen mehrere Timesteps bevor Reward eintritt.\n",
    "(Breakout: Ball zurückschleudern -> dauert bis Ball ankommt. -> mehrere Zwischenzustände)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Einfachste Variante Reinforcement Learning umzusetzten -> Q-Learning.\n",
    "Hierfür werden sogenannte Q-Value oder Action-Values \"geschätzt\" (estimated). Diese weisen einem Zustand der Spielumgebung einen nummerischen Wert für jede mögliche Aktion des Agenten zu. Der Q-Value liefert dann eine \"Abschätzung\" darüber, welche Aktion am wahrscheinlichsten zu dem höchsten zukünftigen Reward führt. Auf diese Weise wird die entsprechende Aktion für den Agenten gewählt.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Die Q-Values sind zu Begin nicht bekannt und müssen irgendwie eingeschätzt werden.\n",
    "- werden mit Null initialisiert und wiederholt aktualisiert, wenn der Agent neue Infos sammelt.\n",
    "- Erziehlt der Agent Punkte, muss der Q-Value aktualisiert werden.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Die einfachste Variante Q-Value zu berechnen: Q-Value = Reward der beobachtet wurde + maximalen Q-Value für den folgenden Zustand des Spiels * Discount-Factor.\n",
    "-> So werden Rewards in näherer Zukunft vom Agent bevorzugt. \n",
    "\n",
    "$Q(s_t, a_t) = r_t + \\gamma * \\max\\limits_{a} Q(s_{t+1}, a)$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Agent verliert/stirbt -> zukünftiger Reward = 0 -> Q-Value = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Policy Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
